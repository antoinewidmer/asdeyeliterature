---
{"dg-publish":true,"permalink":"/literature-review/yu2024/","title":"Video-based Analysis Reveals Atypical Social Gaze in People with Autism Spectrum Disorder","tags":["Computer","Science","-","Machine","Learning","Quantitative","Biology","-","Neurons","and","Cognition"]}
---


## Video-based Analysis Reveals Atypical Social Gaze in People with Autism Spectrum Disorder

> [!Cite]
> Yu, X., Ruan, M., Hu, C., Li, W., Paul, L. K., Li, X., & Wang, S. (2024). _Video-based Analysis Reveals Atypical Social Gaze in People with Autism Spectrum Disorder_ (No. arXiv:2409.00664). arXiv. [https://doi.org/10.48550/arXiv.2409.00664](https://doi.org/10.48550/arXiv.2409.00664)


## üìå Summary

This study examines **atypical social gaze behaviors** in individuals with Autism Spectrum Disorder (ASD) using **video-based analysis** rather than traditional eye-tracking technology. By analyzing third-person **ADOS-2 interview videos**, the researchers extracted gaze-related features using **computer vision models** (OpenFace for gaze tracking, OpenPose for body movement, and Google Speech-to-Text for speech analysis). The study quantitatively assessed **gaze engagement, variance, density maps, and gaze diversion frequency**. A **Random Forest classifier** trained on these features successfully distinguished ASD participants from neurotypical controls, demonstrating the potential of **non-invasive, third-person video analysis for ASD assessment**.


## üî¨ Methods

### Study Design

- **Experimental approach**: Retrospective analysis of **ADOS-2 interview videos**.
- **Perspective**: Third-person (external camera), as opposed to conventional **eye-tracking** or first-person wearable setups.
- **Gaze feature extraction**: **OpenFace**, **OpenPose**, and **Google Speech-to-Text** were used to extract gaze behaviors, body movements, and speech timestamps.

### Participants

- **Total analyzed videos**: 52
    - **ASD participants**: 43 ADOS-2 videos (from the Caltech dataset)
    - **Neurotypical controls**: 9 ADOS-2 videos (from West Virginia University)
- **Subgroup categorization**:
    - **B1=2 Group** (n=37): ASD participants with **atypical eye contact**.
    - **B1=0 Group** (n=6): ASD participants **without** identified gaze impairments.
    - **Control Group** (n=9): Neurotypical individuals.

### Tasks for Participants

- Participants engaged in **Task 6 ("Social Difficulties and Annoyance")** from the **ADOS-2 Module 4**.
- Structured **question-answer interviews** aimed at eliciting **social gaze responses**.
- Examined **gaze behaviors during speaking vs. non-speaking intervals**.

### System Setup and Hardware

|**Component**|**Description**|
|---|---|
|**Video Recording**|ADOS-2 interview videos recorded using standard cameras (Caltech: **720x480, 30fps**, WVU: **1920x1080, 60fps**).|
|**Gaze Tracking**|OpenFace 2.0, an AI-based **face and gaze-tracking** tool, extracted gaze angles and eye positions.|
|**Pose Tracking**|OpenPose detected **body movements** of the examiner to understand participant-examiner interactions.|
|**Speech Analysis**|Google Speech-to-Text extracted **timestamps of speech** to analyze gaze behavior during speaking vs. silent periods.|

### Data Analysis

- **Gaze Metrics Analyzed**:
    1. **Gaze Engagement Ratio**: Percentage of time gaze was directed at the examiner.
    2. **Gaze Variance**: Stability of gaze shifts.
    3. **Gaze Density Maps**: Distribution of gaze across the visual field.
    4. **Gaze Diversion Frequency**: Number of times gaze shifted away from the examiner‚Äôs face per minute.
- **Classification Model**:
    - **Random Forest Classifier** trained on gaze features to distinguish ASD participants from neurotypical controls.
    - Performance measured using **accuracy, precision, recall, and F1-score**.


## üìä Results & Key Findings

### **1. Gaze Engagement**

- **B1=2 (Atypical gaze ASD)**: **41.3% gaze engagement** (significantly lower than controls).
- **B1=0 (Typical gaze ASD)**: **67.7% gaze engagement** (similar to controls).
- **Controls**: **67.2% gaze engagement**.
- Lower **gaze engagement** during **both speaking and non-speaking** intervals in ASD (B1=2 group).

### **2. Gaze Variance**

- ASD participants with **atypical gaze (B1=2)** had **higher gaze instability**.
- ASD participants with **typical gaze (B1=0)** had **lower gaze variance than neurotypical controls**.

### **3. Gaze Distribution and Concentration**

- **B1=2 group** had **wider gaze spread**, suggesting **more dispersed gaze patterns**.
- **B1=0 group** had the **narrowest gaze distribution**, suggesting **more focused gaze patterns**.
- **Control group had the largest gaze concentration area** (8530 pixels vs. 3161 pixels in B1=0 ASD group).

### **4. Gaze Diversion Frequency**

- No significant difference in **gaze diversion frequency** across ASD and control groups.

### **5. Classification Model Performance**

|**Comparison**|**Accuracy**|**Precision**|**Recall**|**F1-score**|
|---|---|---|---|---|
|**ASD (B1=2) vs. Controls**|**83.9%**|81.7%|83.9%|80.9%|
|**ASD (B1=2) vs. ASD (B1=0)**|**83.1%**|74.7%|83.1%|78.1%|
|**ASD (B1=0) vs. Controls**|**71.6%**|78.8%|71.6%|70.8%|

- **Best classification performance**: Differentiating **ASD (B1=2) from controls (83.9% accuracy)**.
- **Challenging classification**: Differentiating **ASD subgroups (B1=2 vs. B1=0, 83.1%)**.


## üîç Related Work

- **Eye-tracking research** shows that ASD individuals exhibit **reduced gaze to social stimuli**.
- **Previous ASD studies** use **wearable eye-trackers** or **first-person camera setups**, whereas this study utilizes **non-intrusive third-person video**.
- **Machine learning in ASD detection** has been explored before, but this study provides a **low-cost, scalable alternative** without requiring specialized equipment.


## üìù Observations

### **Strengths of the Study**

‚úÖ **Non-invasive**: Unlike traditional **eye-tracking systems**, this approach relies on standard video recordings.  
‚úÖ **Scalable and cost-effective**: No need for specialized **eye-tracking hardware**, making it **accessible for large-scale ASD screening**.  
‚úÖ **Real-world settings**: Third-person video captures **naturalistic social behaviors**, which is more **ecologically valid** than controlled lab studies.  
‚úÖ **High classification accuracy**: **83.9% accuracy** in distinguishing ASD from neurotypical controls using **machine learning models**.  
‚úÖ **Insights into ASD subtypes**: Identifies differences between ASD individuals with **atypical gaze (B1=2) vs. typical gaze (B1=0)**.

### **Major Concerns and Challenges**

‚ö†Ô∏è **Small dataset**: Only **52 videos** analyzed (43 ASD, 9 controls), limiting generalizability.  
‚ö†Ô∏è **Resolution limitations**: Videos were **low resolution** (Caltech: 720x480), which may affect **gaze detection accuracy**.  
‚ö†Ô∏è **Limited classifier generalizability**: Struggles to distinguish **ASD subgroups (B1=2 vs. B1=0, 83.1% accuracy)**.  
‚ö†Ô∏è **No ground truth for examiner behavior**: The study **only tracked participants**, not the **examiner‚Äôs gaze or facial expressions**, which could affect results.  
‚ö†Ô∏è **Potential biases in dataset**: Different recording setups at **Caltech (30fps, low resolution) vs. WVU (60fps, high resolution)** could introduce **variability in data quality**.


### **üöÄ Future Directions**

üîπ **Larger dataset**: Include **more neurotypical controls** and **diverse ASD subgroups**.  
üîπ **Higher-resolution videos**: Improve gaze-tracking accuracy using **higher frame rates and multiple camera angles**.  
üîπ **Multimodal analysis**: Combine gaze data with **facial expression and body movement analysis** for a more **comprehensive ASD assessment**.  
üîπ **Home-based screening**: Adapt the approach for **at-home ASD screening** using parent-recorded videos.

